{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06c0d564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x187c80db510>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from scipy.spatial import distance\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.datasets import data, utils, configs\n",
    "from src.datasets.utils import dump_generated_dataset\n",
    "from src.metrics import hungarian_slots_loss\n",
    "from src.utils.training_utils import sample_z_from_latents\n",
    "\n",
    "\n",
    "from torchvision import transforms as transforms\n",
    "\n",
    "import imageio\n",
    "\n",
    "seed = 43\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aba3c949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating images (sampling: diagonal): 100%|███████████████████████████████████| 10000/10000 [01:27<00:00, 113.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a diagonal dataset, to get valid latents\n",
    "n_samples = 10000\n",
    "n_slots = 2\n",
    "default_cfg = configs.SpriteWorldConfig()\n",
    "sample_mode = \"diagonal\"\n",
    "no_overlap = True\n",
    "delta = 0.125\n",
    "\n",
    "diagonal_dataset = data.SpriteWorldDataset(\n",
    "    n_samples,\n",
    "    n_slots,\n",
    "    default_cfg,\n",
    "    sample_mode=sample_mode,\n",
    "    no_overlap=no_overlap,\n",
    "    delta=delta,\n",
    "    transform=transforms.Compose(\n",
    "    [transforms.ToPILImage(), transforms.ToTensor()]),\n",
    "#     z=new_z\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60783d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(diagonal_dataset.x, \"D:/mnt/qb/work/bethge/apanfilov27/object_centric_consistency_project/dsprites/test/random/mixed/images/images_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02eca241",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch.cat([diagonal_dataset.z[:, :, :4], diagonal_dataset.z[:, :, 5:-2]], dim=-1), \"D:/mnt/qb/work/bethge/apanfilov27/object_centric_consistency_project/dsprites/test/random/mixed/latents/latents_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "938e1eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_data(index, dataset):\n",
    "    plt.imshow(dataset[index][0][-1].permute(1, 2, 0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bfdbe01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb72926e6bbb48d59b30959a8ed24720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=33332), Output()), _dom_classes=('widget-int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dispaly_diagonal = lambda index: display_data(index, dataset=diagonal_dataset)\n",
    "\n",
    "num_samples = len(diagonal_dataset)\n",
    "\n",
    "# slider\n",
    "widgets.interact(dispaly_diagonal, index=widgets.IntSlider(min=0, max=num_samples-1, step=1, value=0));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2da8741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_z, indicies = filter_objects(diagonal_dataset.z, max_objects=10000, threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06202125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "print(new_z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c051060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(diagonal_dataset.z, \"D:/mnt/qb/heatmap_dataset/initial_id_latents.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4adc1d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta is too big for 'no_overlap' mode, setting it to 0.08333333333333333.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating images (sampling: diagonal): 100%|██████████████████████████████████████| 2673/2673 [00:38<00:00, 68.78it/s]\n"
     ]
    }
   ],
   "source": [
    "diagonal_dataset = data.SpriteWorldDataset(\n",
    "    len(new_z),\n",
    "    n_slots,\n",
    "    default_cfg,\n",
    "    sample_mode=sample_mode,\n",
    "    no_overlap=no_overlap,\n",
    "    delta=delta,\n",
    "    transform=transforms.Compose(\n",
    "    [transforms.ToPILImage(), transforms.ToTensor()]),\n",
    "    z=new_z\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ec3f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating images (sampling: off_diagonal): 100%|█████████████████████████████████| 5000/5000 [00:44<00:00, 111.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create a off_diagonal dataset, to get valid latents\n",
    "n_samples = 5000\n",
    "n_slots = 2\n",
    "default_cfg = configs.SpriteWorldConfig()\n",
    "sample_mode = \"off_diagonal\"\n",
    "no_overlap = False\n",
    "delta = 0.125\n",
    "\n",
    "off_diagonal_dataset = data.SpriteWorldDataset(\n",
    "    n_samples,\n",
    "    n_slots,\n",
    "    default_cfg,\n",
    "    sample_mode=sample_mode,\n",
    "    no_overlap=no_overlap,\n",
    "    delta=delta,\n",
    "    transform=transforms.Compose(\n",
    "    [transforms.ToPILImage(), transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "# Extract ood_latents and replace their x and y coordinate by diagonal latens\n",
    "no_overlap_z = off_diagonal_dataset.z.clone()\n",
    "no_overlap_z[:, :, :1] = diagonal_dataset.z[:, :, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2bbc8a61",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating images (sampling: off_diagonal):  50%|████████████████▌                | 2501/5000 [00:23<00:23, 106.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[33], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m no_overlap \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m      8\u001B[0m delta \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.125\u001B[39m\n\u001B[1;32m---> 10\u001B[0m no_overlap_off_diagonal_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSpriteWorldDataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_samples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_slots\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdefault_cfg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mno_overlap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mno_overlap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdelta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransforms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompose\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43mtransforms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mToPILImage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransforms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mToTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43mz\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mno_overlap_z\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\git_projects\\bethgelab\\lab_rotation\\object_centric_ood\\notebooks\\..\\src\\datasets\\data.py:136\u001B[0m, in \u001B[0;36mSpriteWorldDataset.__init__\u001B[1;34m(self, n_samples, n_slots, cfg, sample_mode, img_h, img_w, delta, no_overlap, transform, z, **kwargs)\u001B[0m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__generate_ind \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;241m=\u001B[39m environment\u001B[38;5;241m.\u001B[39mEnvironment(\n\u001B[0;32m    129\u001B[0m     task\u001B[38;5;241m=\u001B[39mtasks\u001B[38;5;241m.\u001B[39mNoReward(),\n\u001B[0;32m    130\u001B[0m     action_space\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    133\u001B[0m     max_episode_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m    134\u001B[0m )\n\u001B[1;32m--> 136\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__generate_from_latents\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdetach()\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__update_latents(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mz))\n",
      "File \u001B[1;32mD:\\git_projects\\bethgelab\\lab_rotation\\object_centric_ood\\notebooks\\..\\src\\datasets\\data.py:189\u001B[0m, in \u001B[0;36mSpriteWorldDataset.__generate_from_latents\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    184\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sample_ind \u001B[38;5;129;01min\u001B[39;00m tqdm\u001B[38;5;241m.\u001B[39mtqdm(\n\u001B[0;32m    185\u001B[0m     \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_samples),\n\u001B[0;32m    186\u001B[0m     desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenerating images (sampling: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msample_mode\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    187\u001B[0m ):\n\u001B[0;32m    188\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__generate_ind \u001B[38;5;241m=\u001B[39m sample_ind\n\u001B[1;32m--> 189\u001B[0m     ts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    190\u001B[0m     out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(np\u001B[38;5;241m.\u001B[39marray(ts\u001B[38;5;241m.\u001B[39mobservation[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m\"\u001B[39m]))\n\u001B[0;32m    192\u001B[0m     images[sample_ind] \u001B[38;5;241m=\u001B[39m out\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\spriteworld\\environment.py:78\u001B[0m, in \u001B[0;36mEnvironment.reset\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset_next_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m---> 78\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dm_env\u001B[38;5;241m.\u001B[39mrestart(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobservation\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\spriteworld\\environment.py:138\u001B[0m, in \u001B[0;36mEnvironment.observation\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mobservation\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    137\u001B[0m   state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate()\n\u001B[1;32m--> 138\u001B[0m   observation \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    139\u001B[0m       name: renderer\u001B[38;5;241m.\u001B[39mrender(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mstate)\n\u001B[0;32m    140\u001B[0m       \u001B[38;5;28;01mfor\u001B[39;00m name, renderer \u001B[38;5;129;01min\u001B[39;00m six\u001B[38;5;241m.\u001B[39miteritems(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_renderers)\n\u001B[0;32m    141\u001B[0m   }\n\u001B[0;32m    142\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m observation\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\spriteworld\\environment.py:139\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mobservation\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    137\u001B[0m   state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate()\n\u001B[0;32m    138\u001B[0m   observation \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m--> 139\u001B[0m       name: renderer\u001B[38;5;241m.\u001B[39mrender(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mstate)\n\u001B[0;32m    140\u001B[0m       \u001B[38;5;28;01mfor\u001B[39;00m name, renderer \u001B[38;5;129;01min\u001B[39;00m six\u001B[38;5;241m.\u001B[39miteritems(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_renderers)\n\u001B[0;32m    141\u001B[0m   }\n\u001B[0;32m    142\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m observation\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\spriteworld\\renderers\\pil_renderer.py:89\u001B[0m, in \u001B[0;36mPILRenderer.render\u001B[1;34m(self, sprites, global_state)\u001B[0m\n\u001B[0;32m     87\u001B[0m im \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mnew(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_canvas_size)\n\u001B[0;32m     88\u001B[0m ImageDraw\u001B[38;5;241m.\u001B[39mDraw(im)\u001B[38;5;241m.\u001B[39mpolygon([\u001B[38;5;28mtuple\u001B[39m(v) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m vertices], fill\u001B[38;5;241m=\u001B[39mcolor)\n\u001B[1;32m---> 89\u001B[0m im \u001B[38;5;241m=\u001B[39m \u001B[43mim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_image_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mANTIALIAS\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     90\u001B[0m im \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mflipud(np\u001B[38;5;241m.\u001B[39marray(im))\n\u001B[0;32m     91\u001B[0m ims\u001B[38;5;241m.\u001B[39mappend(im)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\PIL\\Image.py:2082\u001B[0m, in \u001B[0;36mImage.resize\u001B[1;34m(self, size, resample, box, reducing_gap)\u001B[0m\n\u001B[0;32m   2074\u001B[0m             \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mreduce(\u001B[38;5;28mself\u001B[39m, factor, box\u001B[38;5;241m=\u001B[39mreduce_box)\n\u001B[0;32m   2075\u001B[0m         box \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2076\u001B[0m             (box[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_x,\n\u001B[0;32m   2077\u001B[0m             (box[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_y,\n\u001B[0;32m   2078\u001B[0m             (box[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_x,\n\u001B[0;32m   2079\u001B[0m             (box[\u001B[38;5;241m3\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_y,\n\u001B[0;32m   2080\u001B[0m         )\n\u001B[1;32m-> 2082\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbox\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Step 3: Make OOD no_overlap dataset\n",
    "\n",
    "n_samples = 5000\n",
    "n_slots = 2\n",
    "default_cfg = configs.SpriteWorldConfig()\n",
    "sample_mode = \"off_diagonal\"\n",
    "no_overlap = False\n",
    "delta = 0.125\n",
    "\n",
    "no_overlap_off_diagonal_dataset = data.SpriteWorldDataset(\n",
    "    n_samples,\n",
    "    n_slots,\n",
    "    default_cfg,\n",
    "    sample_mode=sample_mode,\n",
    "    no_overlap=no_overlap,\n",
    "    delta=delta,\n",
    "    transform=transforms.Compose(\n",
    "    [transforms.ToPILImage(), transforms.ToTensor()]),\n",
    "    z=no_overlap_z\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec8edaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating images (sampling: off_diagonal): 100%|███████████████████████████████| 10000/10000 [01:32<00:00, 107.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 2 - alternative:\n",
    "# Generate off_diagonal dataset and reject the pairs with close x-coordinates\n",
    "\n",
    "n_samples = 10000\n",
    "n_slots = 2\n",
    "default_cfg = configs.SpriteWorldConfig()\n",
    "sample_mode = \"off_diagonal\"\n",
    "no_overlap = False\n",
    "delta = 0.125\n",
    "\n",
    "off_diagonal_dataset = data.SpriteWorldDataset(\n",
    "    n_samples,\n",
    "    n_slots,\n",
    "    default_cfg,\n",
    "    sample_mode=sample_mode,\n",
    "    no_overlap=no_overlap,\n",
    "    delta=delta,\n",
    "    transform=transforms.Compose(\n",
    "    [transforms.ToPILImage(), transforms.ToTensor()])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd413562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering objects:  75%|█████████████████████████████████████████▍             | 7524/10000 [00:00<00:00, 20246.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 2.1 filtering\n",
    "no_overlap_z = filter_objects(off_diagonal_dataset.z, max_objects=5000, threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc72d693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating images (sampling: off_diagonal): 100%|█████████████████████████████████| 5000/5000 [00:38<00:00, 128.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - alternative, filter the latents and create new data\n",
    "\n",
    "n_samples = 5000\n",
    "n_slots = 2\n",
    "default_cfg = configs.SpriteWorldConfig()\n",
    "sample_mode = \"off_diagonal\"\n",
    "no_overlap = False\n",
    "delta = 0.125\n",
    "\n",
    "no_overlap_off_diagonal_dataset = data.SpriteWorldDataset(\n",
    "    n_samples,\n",
    "    n_slots,\n",
    "    default_cfg,\n",
    "    sample_mode=sample_mode,\n",
    "    no_overlap=no_overlap,\n",
    "    delta=delta,\n",
    "    transform=transforms.Compose(\n",
    "    [transforms.ToPILImage(), transforms.ToTensor()]),\n",
    "    z=no_overlap_z\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b2177254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_samples = len(no_overlap_off_diagonal_dataset)\n",
    "\n",
    "# Convert images to numpy arrays and add them to the images list\n",
    "images = []\n",
    "for i in range(num_samples):\n",
    "    img = no_overlap_off_diagonal_dataset[i][0][-1].permute(1, 2, 0).numpy()\n",
    "    \n",
    "    # Scaling the image data to [0, 255] and convert to uint8\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    \n",
    "    images.append(img)\n",
    "\n",
    "# Save images as a GIF\n",
    "imageio.mimsave('output.gif', images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9554c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_objects(latents, max_objects=5000, threshold=0.3, sort=False):\n",
    "    \"\"\"\n",
    "    Filter objects based on their Euclidean distance.\n",
    "    Args:\n",
    "        latents: Tensor of shape (batch_size, n_slots, n_latents)\n",
    "        max_objects: Number of objects to keep at most\n",
    "        threshold: Distance threshold\n",
    "        sort: Whether to sort the objects by distance\n",
    "    \"\"\"\n",
    "    N, slots, _ = latents.size()\n",
    "    mask = torch.zeros(N, dtype=bool)\n",
    "\n",
    "    # Compute Euclidean distance for each pair of slots in each item\n",
    "    for n in range(N):\n",
    "        slots_distances = torch.cdist(latents[n, :, :2], latents[n, :, :2], p=2)\n",
    "        slots_distances.fill_diagonal_(float('inf'))  # Ignore distance to self\n",
    "\n",
    "        # Consider an object as \"close\" if its minimal distance to any other object is below the threshold\n",
    "        min_distance = slots_distances.min().item()\n",
    "        if min_distance >= threshold:\n",
    "            mask[n] = True\n",
    "\n",
    "    # If all objects are \"close\", print a message and return\n",
    "    if not torch.any(mask):\n",
    "        print(\"No objects were found that meet the distance threshold.\")\n",
    "        return None, []\n",
    "\n",
    "    # Apply the mask to the latents\n",
    "    filtered_objects = latents[mask]\n",
    "    filtered_indices = torch.arange(N)[mask]\n",
    "\n",
    "    # If the number of filtered objects exceeds the maximum, truncate them\n",
    "    if filtered_objects.size(0) > max_objects:\n",
    "        filtered_objects = filtered_objects[:max_objects]\n",
    "        filtered_indices = filtered_indices[:max_objects]\n",
    "\n",
    "    if sort:\n",
    "        # Sort the filtered objects by minimum distance to any other object\n",
    "        min_distances = torch.zeros(mask.sum().item())\n",
    "        for i, n in enumerate(torch.where(mask)[0]):\n",
    "            slots_distances = torch.cdist(latents[n], latents[n], p=2)\n",
    "            slots_distances.fill_diagonal_(float('inf'))\n",
    "            min_distances[i] = slots_distances.min().item()\n",
    "\n",
    "        indices = torch.argsort(min_distances)\n",
    "        filtered_objects = filtered_objects[indices]\n",
    "        filtered_indices = filtered_indices[indices]\n",
    "\n",
    "    return filtered_objects, filtered_indices.tolist()\n",
    "\n",
    "\n",
    "\n",
    "def sample_z_from_latents_no_overlap(\n",
    "    gt_z, hat_z, gt_figures, hat_figures, device, n_samples=1024\n",
    "):\n",
    "    _, transposed_indices = hungarian_slots_loss(\n",
    "        gt_figures.view(gt_figures.shape[0], gt_figures.shape[1], -1),\n",
    "        hat_figures.view(hat_figures.shape[0], hat_figures.shape[1], -1),\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    transposed_indices = transposed_indices.to(device)\n",
    "\n",
    "    hat_z_permuted = hat_z.gather(\n",
    "        1,\n",
    "        transposed_indices[:, :, 1].unsqueeze(-1).expand(-1, -1, hat_z.shape[-1]),\n",
    "    )\n",
    "    gt_z_flatten = gt_z.view(-1, gt_z.shape[2])\n",
    "    z_sampled, indices = sample_z_from_latents(hat_z_permuted.detach(), n_samples=20000)\n",
    "\n",
    "    # reshape z_flatten with indices\n",
    "    z_flatten = gt_z_flatten[indices].reshape(-1, gt_z.shape[1], gt_z.shape[2])\n",
    "\n",
    "    z_sampled, selected_pairs_indices = filter_objects(z_flatten, max_objects=n_samples, threshold=0.3)\n",
    "\n",
    "    return z_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "101fe524",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sampled = sample_z_from_latents_no_overlap(diagonal_dataset.z, diagonal_dataset.z, diagonal_dataset.x[:, :-1, ...], diagonal_dataset.x[:, :-1, ...], \"cpu\", n_samples=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad9077c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 2, 8])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eea0db98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating images (sampling: off_diagonal): 100%|█████████████████████████████████| 5000/5000 [00:49<00:00, 100.02it/s]\n"
     ]
    }
   ],
   "source": [
    "test = data.SpriteWorldDataset(\n",
    "    len(z_sampled),\n",
    "    n_slots,\n",
    "    default_cfg,\n",
    "    sample_mode=\"off_diagonal\",\n",
    "    no_overlap=True,\n",
    "    delta=delta,\n",
    "    transform=transforms.Compose(\n",
    "    [transforms.ToPILImage(), transforms.ToTensor()]),\n",
    "    z=z_sampled\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0d48151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b206a3249ba4e16b8080e2860635f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=4999), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_data(index):\n",
    "    plt.imshow(test[index][0][-1].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "num_samples = len(test)\n",
    "\n",
    "# slider\n",
    "widgets.interact(display_data, index=widgets.IntSlider(min=0, max=num_samples-1, step=1, value=0));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "076aeca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(z_sampled)\n",
    "\n",
    "# Convert images to numpy arrays and add them to the images list\n",
    "images = []\n",
    "for i in range(num_samples):\n",
    "    img = test[i][0][-1].permute(1, 2, 0).numpy()\n",
    "    \n",
    "    # Scaling the image data to [0, 255] and convert to uint8\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    \n",
    "    images.append(img)\n",
    "\n",
    "# Save images as a GIF\n",
    "imageio.mimsave('output.gif', images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe90ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:00, 6555.41it/s]\n"
     ]
    }
   ],
   "source": [
    "dump_generated_dataset(diagonal_dataset, \"D:/mnt/qb/work/bethge/apanfilov27/object_centric_consistency_project/dsprites/test/diagonal/4_objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5bd6e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:/mnt/qb/work/bethge/apanfilov27/object_centric_consistency_project/dsprites/train/diagonal/4_objects/latents/latents.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24d89418",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_4 = torch.load(\"D:/mnt/qb/work/bethge/apanfilov27/object_centric_consistency_project/dsprites/test/off_diagonal/4_objects/latents/latents.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8557b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(latents_4[:1666, ...], \"D:/mnt/qb/work/bethge/apanfilov27/object_centric_consistency_project/dsprites/test/off_diagonal/mixed/latents/latents_4.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b4c0b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965deeb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latens.pt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('D:/mnt/qb/work/bethge/apanfilov27/object_centric_consistency_project/dsprites/train/diagonal/4_objects/latents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc2191c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f5f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6213cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[0.9041,  0.0196], [-0.3108, 0]], [[-0.4821,  1.059], [-0.4821,  1.059]]])\n",
    "b = torch.tensor([[[-2.1763, -0.4713], [-0.6986,  1.3702]], [[-0.4821,  1.059], [-0.4821,  1.059]]])\n",
    "\n",
    "\n",
    "pairwise_cost = torch.cdist(a, b, p=2).transpose(-1, -2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2515c9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.1193, 1.9241],\n",
       "         [2.0959, 1.4240]],\n",
       "\n",
       "        [[0.0000, 0.0000],\n",
       "         [0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d104247",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.array(\n",
    "        list(map(linear_sum_assignment, pairwise_cost))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2127334e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x1d93ae0f670>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(linear_sum_assignment, pairwise_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40688c20",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cost matrix is infeasible",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m cost \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([[np\u001B[38;5;241m.\u001B[39minf, np\u001B[38;5;241m.\u001B[39minf, \u001B[38;5;241m4\u001B[39m], \n\u001B[0;32m      2\u001B[0m                  [np\u001B[38;5;241m.\u001B[39minf, np\u001B[38;5;241m.\u001B[39minf, np\u001B[38;5;241m.\u001B[39minf], \n\u001B[0;32m      3\u001B[0m                  [np\u001B[38;5;241m.\u001B[39minf, \u001B[38;5;241m4\u001B[39m, np\u001B[38;5;241m.\u001B[39minf]])\n\u001B[1;32m----> 4\u001B[0m row_ind, col_ind \u001B[38;5;241m=\u001B[39m \u001B[43mlinear_sum_assignment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcost\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(cost)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(col_ind)\n",
      "\u001B[1;31mValueError\u001B[0m: cost matrix is infeasible"
     ]
    }
   ],
   "source": [
    "cost = np.array([[np.inf, np.inf, 4], \n",
    "                 [np.inf, np.inf, np.inf], \n",
    "                 [np.inf, 4, np.inf]])\n",
    "row_ind, col_ind = linear_sum_assignment(cost)\n",
    "print(cost)\n",
    "print(col_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bdc187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load tensors\n",
    "tensor1 = torch.load('images_2.pt')\n",
    "tensor2 = torch.load('images_3.pt')\n",
    "tensor3 = torch.load('images_4.pt')\n",
    "\n",
    "# Make sure the tensors are in the correct shape\n",
    "assert tensor1.shape == (33333, 3, 3, 64, 64)\n",
    "assert tensor2.shape == (33333, 4, 3, 64, 64)\n",
    "assert tensor3.shape == (33333, 5, 3, 64, 64)\n",
    "\n",
    "# Create tensors of zeros with the desired final shape\n",
    "zeros_1 = torch.zeros((33333, 4, 3, 64, 64))\n",
    "zeros_2 = torch.zeros((33333, 4, 3, 64, 64))\n",
    "zeros_3 = torch.zeros((33333, 4, 3, 64, 64))\n",
    "\n",
    "# Fill in the parts of the zeros tensor with the loaded tensors\n",
    "zeros_1[:, :2, ] = tensor1\n",
    "zeros_2[:, :3, :] = tensor2\n",
    "zeros_3[:, :4, :] = tensor3\n",
    "\n",
    "# Concatenate along the first dimension\n",
    "concat_tensor = torch.cat((zeros_1, zeros_2, zeros_3), dim=0)\n",
    "\n",
    "# Check the shape of the result\n",
    "assert concat_tensor.shape == (99999, 4, 5)\n",
    "\n",
    "# Save the new tensor\n",
    "torch.save(concat_tensor, 'images.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
