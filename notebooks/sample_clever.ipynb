{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = np.load(\"/home/thaddaus/code/ocood/clevr_latents.npy\")\n",
    "# imgs = np.load(\"/mnt/qb/work/bethge/jbrady61/clevr_data/clevr_obs.npy\")\n",
    "nums = np.load(\"/home/thaddaus/code/ocood/clevr_num_objects.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4, 5, 6]), array([   14,  1040, 13145, 12875, 13032, 13377]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution over number of objects\n",
    "np.unique(nums, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0.] [1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# normalize latents\n",
    "mins = lats.min(axis=(0,1))\n",
    "maxs = lats.max(axis=(0,1))\n",
    "lats_norm = (lats - mins) / (maxs - mins)\n",
    "print(lats_norm.min(axis=(0,1)), lats_norm.max(axis=(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.224744871391589"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(6) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ID_OOD_splits(\n",
    "    latents: np.ndarray, delta: float\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    All distances from the diagonal are bigger than delta.\n",
    "    Opposite case of __sample_delta_diagonal_cube.\n",
    "\n",
    "    Rejection sampling used as the algorithm.\n",
    "    \"\"\"\n",
    "    indcs_ID = np.ndarray((0))\n",
    "    indcs_OOD = np.ndarray((0))\n",
    "    indcs_ID_per_n = []\n",
    "    indcs_OOD_per_n = []\n",
    "\n",
    "    # we need to separately sample for different numbers of objects\n",
    "    for n_slots in range(1, latents.shape[1] + 1):\n",
    "        max_delta = np.sqrt(n_slots) / 2\n",
    "\n",
    "        # we only need to keep track of the indeces\n",
    "        at_most_n_slots = np.all(lats[:, n_slots:, :] == 0, axis=(1, 2))\n",
    "        at_least_n_slots = np.any(lats[:, n_slots - 1, :] != 0, axis=1)\n",
    "        indcs = np.nonzero(at_most_n_slots & at_least_n_slots)[0]\n",
    "\n",
    "        latents_n_slots = latents[indcs][:, :n_slots]\n",
    "\n",
    "        # define the diagonal\n",
    "        diag_unit = np.ones(n_slots) / np.sqrt(n_slots)\n",
    "        # calculate the projection onto the diagonal (and from there the distance) along the\n",
    "        # `slots`-dimension since one diagonal contains the i-th latent of each slot\n",
    "        diag_scalar_component = np.dot(latents_n_slots.transpose(0, 2, 1), diag_unit)\n",
    "        diag_component = diag_scalar_component[:, None, :] * diag_unit[None, :, None]\n",
    "        orth_component = latents_n_slots - diag_component\n",
    "        orth_component_norm = np.linalg.norm(orth_component, axis=1)\n",
    "\n",
    "        # diag = np.ones((latents_n_slots.shape[0], n_slots, latents.shape[2]))\n",
    "\n",
    "        # get distance to line defined by diagonal along each latent dimension\n",
    "        # ort_vec = latents_n_slots - diag * (latents_n_slots * diag).sum(axis=1, keepdims=True)\\\n",
    "        #         / (diag * diag).sum(axis=1, keepdims=True)\n",
    "        \n",
    "        # find ID/OOD points based on distance to diagonal\n",
    "        mask_ID = np.all(orth_component_norm <= delta * max_delta, axis=1)\n",
    "        mask_OOD = np.any(orth_component_norm > delta * max_delta, axis=1)\n",
    "\n",
    "        # print(mask_ID.sum(), mask_OOD.sum(), mask_OOD.sum() + mask_ID.sum() == len(indcs))\n",
    "        \n",
    "        indcs_ID = np.append(indcs_ID, indcs[mask_ID])\n",
    "        indcs_OOD = np.append(indcs_OOD, indcs[mask_OOD])\n",
    "        \n",
    "        indcs_ID_per_n.append(mask_ID.sum())\n",
    "        indcs_OOD_per_n.append(mask_OOD.sum())\n",
    "\n",
    "    return indcs_ID, indcs_OOD, indcs_ID_per_n, indcs_OOD_per_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inf, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[14, 0, 0, 0, 0, 0]\n",
      "14 53469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_938/879517513.py:5: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  print([indcs_ID_per_n[i] / indcs_OOD_per_n[i] for i in range(6)])\n"
     ]
    }
   ],
   "source": [
    "# delta in [0 .. 1]\n",
    "indcs_ID, indcs_OOD, indcs_ID_per_n, indcs_OOD_per_n = get_ID_OOD_splits(lats_norm, 0)\n",
    "\n",
    "print([indcs_ID_per_n[i] / indcs_OOD_per_n[i] for i in range(6)])\n",
    "print(indcs_ID_per_n)\n",
    "print(sum(indcs_ID_per_n), sum(indcs_OOD_per_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity checks for ID and OOD sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_delta_diagonal_cube(\n",
    "    n_samples: int, n_slots: int, n_latents: int, delta: float, oversampling: int = 10\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sample near the diagonal in latent space i.e. all distances from the diagonal are less than delta.\n",
    "\n",
    "    Algorithm:\n",
    "        1. Draw points on the diagonal of [0, 1)^(n_slots, n_latents) cube.\n",
    "        2. For every latent draw uniformly noise from n_slots-dimensional ball. For drawing uniformly inside the ball we\n",
    "            use the following theorem (http://compneuro.uwaterloo.ca/files/publications/voelker.2017.pdf):\n",
    "            if point uniformly sampled from the (n+1)-sphere, then n-first coordinates are uniformly sampled from the n-ball.\n",
    "        3. Project sampled inside-ball points to the hyperplane perpendicular to the diagonal and normalize them\n",
    "            (this gives us points on (n_slots-2)-sphere embedded in n_slots-space).\n",
    "        4. Get final points by adding the diagonal point to the projected points.\n",
    "        5. Keep only points inside the [0, 1)^(n_slots, n_latents) cube.\n",
    "    \"\"\"\n",
    "    _n = oversampling * n_samples\n",
    "    z_out = torch.Tensor(0, n_slots, n_latents)\n",
    "    while z_out.shape[0] < n_samples:\n",
    "        # sample randomly on diagonal\n",
    "        z_sampled = torch.repeat_interleave(\n",
    "            torch.rand(_n, n_latents), n_slots, dim=0\n",
    "        ).reshape(_n, n_slots, n_latents)\n",
    "\n",
    "        # sample noise from n_slots-ball\n",
    "        noise = torch.randn(_n, n_slots + 2, n_latents)\n",
    "        noise = noise / torch.norm(noise, dim=1, keepdim=True)  # points on n-sphere\n",
    "        noise = noise[:, :n_slots, :]  # remove two last points\n",
    "\n",
    "        # project to hyperplane perpendicular to diagonal\n",
    "        ort_vec = noise - z_sampled * (noise * z_sampled).sum(axis=1, keepdim=True) / (\n",
    "            z_sampled * z_sampled\n",
    "        ).sum(axis=1, keepdim=True)\n",
    "        ort_vec /= torch.norm(ort_vec, p=2, dim=1, keepdim=True)\n",
    "\n",
    "        # final step\n",
    "        # why n - 1 here? because we sample\n",
    "        # \"radius\" not in the original space, but in the embedded\n",
    "        final = z_sampled + (\n",
    "            ort_vec\n",
    "            * torch.pow(torch.rand([_n, 1, n_latents]), 1 / (n_slots - 1))\n",
    "            * delta\n",
    "        )\n",
    "\n",
    "        # only keep samples inside [0, 1]^{kÃ—l}\n",
    "        mask = ((final - 0.5).abs() <= 0.5).flatten(1).all(1)\n",
    "        idx = mask.nonzero().squeeze(1)\n",
    "\n",
    "        z_out = torch.cat([z_out, final[idx]])\n",
    "    z_out = z_out[:n_samples]\n",
    "    return z_out[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 99995 100000\n",
      "True\n",
      "True\n",
      "1000 0 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fcbf06271f0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 100000\n",
    "n_slots = 6\n",
    "n_latents = 6\n",
    "delta = 0.5  # in [0 .. sqrt(n_slots)/2]  or  [0 .. sqrt(n_slots * n_latents) / 2] for _norm\n",
    "\n",
    "z = np.random.rand(n, n_slots, n_latents)\n",
    "\n",
    "# if a_1=[a_1_1, ..., a_1_n_latents] is a single slot with dim=n_slots\n",
    "# then the entire space is z=[[a_1_1, ..., a_1_n_latents], ..., [a_n_slots_1, ..., a_n_slots_n_latents]] with dim=n_latents\n",
    "# on the diagonal, a_1 can assume all possible values, and the other slots must be equal to it\n",
    "#   d = [a_1, a_1, a_1, ...]\n",
    "# since a_1_i are completely independent of each other, there's essentially n_latents independent diagonals with dim=n_slots\n",
    "#   d_:_i = [a_1_i, a_1_i, ..., a_1_i]  with  i in [1 .. n_latents]\n",
    "# for a datapoint to lie \"near to the diagonal\", it must be near to all component diagonals, i.e.\n",
    "#   [a_1_i, a_2_i, ..., a_n_slots_i] must be close to d_:_i for all i\n",
    "# i.e. a_:_i - a^d_:_i < delta\n",
    "# with a^d_:_i = a_:_i * diag_norm\n",
    "\n",
    "diag_unit = np.ones(n_slots) / np.sqrt(n_slots)\n",
    "# calculate the projection onto the diagonal (and from there the distance) along the\n",
    "# `slots`-dimension since one diagonal contains the i-th latent of each slot\n",
    "diag_scalar_component = np.dot(z.transpose(0, 2, 1), diag_unit)\n",
    "diag_component = diag_scalar_component[:, None, :] * diag_unit[None, :, None]\n",
    "orth_component = z - diag_component\n",
    "orth_component_norm = np.linalg.norm(orth_component, axis=1)\n",
    "\n",
    "# ID points lie within `delta` from each diagonal\n",
    "mask_ID = np.all(orth_component_norm <= delta, axis=1)\n",
    "# mask_ID_norm = np.linalg.norm(z_orth_component_norm, axis=1) <= delta\n",
    "# OOD points have at least one component in one latent that lies outside `delta` from\n",
    "# the diagonal\n",
    "mask_OOD_any = np.any(orth_component_norm > delta, axis=1)\n",
    "mask_OOD_all = np.all(orth_component_norm > delta, axis=1)\n",
    "# mask_OOD_norm = np.linalg.norm(z_orth_component_norm, axis=1) > delta\n",
    "\n",
    "n_ID = mask_ID.sum()\n",
    "# n_ID_norm = mask_ID_norm.sum()\n",
    "n_OOD_any = mask_OOD_any.sum()\n",
    "n_OOD_all = mask_OOD_all.sum()\n",
    "# n_OOD_norm = mask_OOD_norm.sum()\n",
    "# print(n_ID, n_OOD_all, n_OOD_any, n_OOD_all / n_OOD_any)\n",
    "print(n_ID, n_OOD_any, n_ID + n_OOD_any)\n",
    "# print(n_ID_norm, n_OOD_norm, n_ID_norm + n_OOD_norm)\n",
    "\n",
    "# double-check that all points in .all are in .any\n",
    "print((mask_OOD_any & mask_OOD_all).sum() == mask_OOD_all.sum())\n",
    "\n",
    "# double-check that OOD_any and ID together contain all points\n",
    "print((mask_ID | mask_OOD_any).sum() == n)\n",
    "\n",
    "# fig = plt.figure(figsize=(12, 12))\n",
    "# ax = fig.add_subplot(projection='3d')\n",
    "# ax.scatter(orth_component_norm[mask_OOD_any][:, 0], orth_component_norm[mask_OOD_any][:, 1], orth_component_norm[mask_OOD_any][:, 2])\n",
    "# ax.scatter(z_orth_component_norm[mask_OOD_all][:, 0], z_orth_component_norm[mask_OOD_all][:, 1], z_orth_component_norm[mask_OOD_all][:, 2])\n",
    "\n",
    "# check that ID samples don't contain any OOD samples\n",
    "z_ID = sample_delta_diagonal_cube(1000, n_slots, n_latents, delta+0.00001).numpy()\n",
    "\n",
    "diag_unit = np.ones(n_slots) / np.sqrt(n_slots)\n",
    "# calculate the projection onto the diagonal (and from there the distance) along the\n",
    "# `slots`-dimension since one diagonal contains the i-th latent of each slot\n",
    "diag_scalar_component = np.dot(z_ID.transpose(0, 2, 1), diag_unit)\n",
    "diag_component = diag_scalar_component[:, None, :] * diag_unit[None, :, None]\n",
    "orth_component = z_ID - diag_component\n",
    "orth_component_norm = np.linalg.norm(orth_component, axis=1)\n",
    "\n",
    "mask_ID = np.all(orth_component_norm <= delta, axis=1)\n",
    "# mask_ID_norm = np.linalg.norm(z_orth_component_norm, axis=1) <= delta\n",
    "mask_OOD_any = np.any(orth_component_norm > delta, axis=1)\n",
    "# mask_OOD_norm = np.linalg.norm(z_orth_component_norm, axis=1) > delta\n",
    "\n",
    "n_ID = mask_ID.sum()\n",
    "# n_ID_norm = mask_ID_norm.sum()\n",
    "n_OOD_any = mask_OOD_any.sum()\n",
    "# n_OOD_norm = mask_OOD_norm.sum()\n",
    "\n",
    "print(n_ID, n_OOD_any, n_ID + n_OOD_any)\n",
    "# print(n_ID_norm, n_OOD_norm, n_ID_norm + n_OOD_norm)\n",
    "\n",
    "ax.scatter(orth_component_norm[:, 0], orth_component_norm[:, 1], orth_component_norm[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_delta_diagonal_cube(\n",
    "    n_samples: int, n_slots: int, n_latents: int, delta: float, oversampling: int = 100\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sample near the diagonal in latent space i.e. all distances from the diagonal are less than delta.\n",
    "\n",
    "    Algorithm:\n",
    "        1. Draw points on the diagonal of [0, 1)^(n_slots, n_latents) cube.\n",
    "        2. For every latent draw uniformly noise from n_slots-dimensional ball. For drawing uniformly inside the ball we\n",
    "            use the following theorem (http://compneuro.uwaterloo.ca/files/publications/voelker.2017.pdf):\n",
    "            if point uniformly sampled from the (n+1)-sphere, then n-first coordinates are uniformly sampled from the n-ball.\n",
    "        3. Project sampled inside-ball points to the hyperplane perpendicular to the diagonal and normalize them\n",
    "            (this gives us points on (n_slots-2)-sphere embedded in n_slots-space).\n",
    "        4. Get final points by adding the diagonal point to the projected points.\n",
    "        5. Keep only points inside the [0, 1)^(n_slots, n_latents) cube.\n",
    "    \"\"\"\n",
    "    _n = oversampling * n_samples\n",
    "    z_out = torch.Tensor(0, n_slots, n_latents)\n",
    "    while z_out.shape[0] < n_samples:\n",
    "        # sample randomly on diagonal\n",
    "        z_sampled = torch.repeat_interleave(\n",
    "            torch.rand(_n, n_latents), n_slots, dim=0\n",
    "        ).reshape(_n, n_slots, n_latents)\n",
    "\n",
    "        # sample noise from n_slots-ball\n",
    "        noise = torch.randn(_n, n_slots + 2, n_latents)\n",
    "        noise = noise / torch.norm(noise, dim=1, keepdim=True)  # points on n-sphere\n",
    "        noise = noise[:, :n_slots, :]  # remove two last points\n",
    "\n",
    "        # project to hyperplane perpendicular to diagonal\n",
    "        ort_vec = noise - z_sampled * (noise * z_sampled).sum(axis=1, keepdim=True) / (\n",
    "            z_sampled * z_sampled\n",
    "        ).sum(axis=1, keepdim=True)\n",
    "        ort_vec /= torch.norm(ort_vec, p=2, dim=1, keepdim=True)\n",
    "\n",
    "        # final step\n",
    "        # why n - 1 here? because we sample\n",
    "        # \"radius\" not in the original space, but in the embedded\n",
    "        final = z_sampled + (\n",
    "            ort_vec\n",
    "            * torch.pow(torch.rand([_n, 1, n_latents]), 1 / (n_slots - 1))\n",
    "            * delta\n",
    "        )\n",
    "\n",
    "        # only keep samples inside [0, 1]^{kÃ—l}\n",
    "        mask = ((final - 0.5).abs() <= 0.5).flatten(1).all(1)\n",
    "        idx = mask.nonzero().squeeze(1)\n",
    "\n",
    "        z_out = torch.cat([z_out, final[idx]])\n",
    "    z_out = z_out[:n_samples]\n",
    "    return z_out[:n_samples]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cood",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
