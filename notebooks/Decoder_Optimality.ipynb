{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23153e18370>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "from src.datasets import data, utils, configs\n",
    "from torchvision import transforms as transforms\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating images (sampling: diagonal): 100%|█████████████████████████████████████| 5000/5000 [00:46<00:00, 107.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a diagonal dataset, to get valid latents\n",
    "n_samples = 5000\n",
    "n_slots = 2\n",
    "default_cfg = configs.SpriteWorldConfig()\n",
    "sample_mode = \"diagonal\"\n",
    "no_overlap = True\n",
    "delta = 0.125\n",
    "\n",
    "diagonal_dataset = data.SpriteWorldDataset(\n",
    "    n_samples,\n",
    "    n_slots,\n",
    "    default_cfg,\n",
    "    sample_mode=sample_mode,\n",
    "    no_overlap=no_overlap,\n",
    "    delta=delta,\n",
    "    transform=transforms.Compose(\n",
    "    [transforms.ToPILImage(), transforms.ToTensor()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a dataloader\n",
    "scale = torch.FloatTensor(\n",
    "    [rng.max - rng.min for rng in default_cfg.get_ranges().values()]\n",
    ").reshape(1, 1, -1)\n",
    "scale = torch.cat([scale[:, :, :-4], scale[:, :, -3:-2]], dim=-1)\n",
    "\n",
    "\n",
    "min_offset = torch.FloatTensor(\n",
    "    [rng.min for rng in default_cfg.get_ranges().values()]\n",
    ").reshape(1, 1, -1)\n",
    "min_offset = torch.cat([min_offset[:, :, :-4], min_offset[:, :, -3:-2]], dim=-1)\n",
    "\n",
    "batch_size = 128\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    diagonal_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: utils.collate_fn_normalizer(b, min_offset, scale),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating images (sampling: diagonal): 100%|█████████████████████████████████████| 5000/5000 [00:45<00:00, 109.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Make a permutation for every batch and create permuted dataset\n",
    "perms = []\n",
    "batch_len_accum = 0\n",
    "for _, true_latents in loader:\n",
    "    batch_len = true_latents.shape[0]\n",
    "    perm = torch.randperm(batch_len) + batch_len_accum\n",
    "    perms.append(perm)\n",
    "    batch_len_accum += batch_len\n",
    "\n",
    "perms_concated = torch.cat(perms)\n",
    "\n",
    "permuted_latents = torch.cat([diagonal_dataset.z[:, 0].unsqueeze(1), diagonal_dataset.z[perms_concated, 1].unsqueeze(1)], dim=1)\n",
    "\n",
    "permuted_dataset = data.SpriteWorldDataset(\n",
    "    n_samples,\n",
    "    n_slots,\n",
    "    default_cfg,\n",
    "    sample_mode=sample_mode,\n",
    "    no_overlap=no_overlap,\n",
    "    delta=delta,\n",
    "    z=permuted_latents, # here we using the provided latents instead of sampling\n",
    "    transform=transforms.Compose(\n",
    "    [transforms.ToPILImage(), transforms.ToTensor()])\n",
    "\n",
    ")\n",
    "\n",
    "permuted_dataloader = torch.utils.data.DataLoader(\n",
    "    permuted_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: utils.collate_fn_normalizer(b, min_offset, scale),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SlotAttentionAutoEncoder(\n",
       "  (encoder_cnn): SlotAttentionEncoder(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (conv4): Conv2d(32, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (encoder_pos): SoftPositionEmbed(\n",
       "      (embedding): Linear(in_features=4, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder_cnn): SlotAttentionDecoder(\n",
       "    (conv_list): ModuleList(\n",
       "      (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (decoder_pos): SoftPositionEmbed(\n",
       "      (embedding): Linear(in_features=4, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (slot_attention): SlotAttention(\n",
       "    (to_q): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (to_k): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (to_v): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (gru): GRUCell(16, 16)\n",
       "    (fc1): Linear(in_features=16, out_features=64, bias=True)\n",
       "    (fc2): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (norm_input): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm_slots): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm_pre_ff): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Load the model\n",
    "from src.models import base_models, slot_attention\n",
    "\n",
    "checkpoint_path = (\"D:/git_projects/bethgelab/lab_rotation/object_centric_ood/notebooks/models/\"\n",
    "                   \"SlotAttention, diagonal, 2 objects/\"\n",
    "                   \"2027.pt\")\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "\n",
    "#SlotAttention\n",
    "encoder = slot_attention.SlotAttentionEncoder(\n",
    "    resolution=(64, 64),\n",
    "    hid_dim=16,\n",
    "    ch_dim=32,\n",
    "    dataset_name=\"dsprites\",\n",
    ")\n",
    "decoder = slot_attention.SlotAttentionDecoder(\n",
    "    hid_dim=16,\n",
    "    ch_dim=32,\n",
    "    resolution=(64, 64),\n",
    "    dataset_name=\"dsprites\",\n",
    ")\n",
    "model = slot_attention.SlotAttentionAutoEncoder(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    num_slots=2,\n",
    "    num_iterations=3,\n",
    "    hid_dim=16,\n",
    "    dataset_name=\"dsprites\",\n",
    ")\n",
    "\n",
    "decoder_hook = model.decode\n",
    "\n",
    "# SlotMLPAdditive\n",
    "# model = base_models.SlotMLPAdditive(3, 2, 16)\n",
    "# decoder_hook = model.decoder\n",
    "\n",
    "\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Get the latents for the original dataset and reshuffle them\n",
    "from src.metrics import hungarian_slots_loss\n",
    "\n",
    "latents = []\n",
    "with torch.no_grad():\n",
    "    for images, true_latents in loader:\n",
    "        true_figures = images[:, :-1, ...]\n",
    "        images = images[:, -1, ...].squeeze(1)\n",
    "\n",
    "        output = model(images)\n",
    "        predicted_figures = output[\"reconstructed_figures\"]\n",
    "        \n",
    "        figures_reshaped = true_figures.view(true_figures.shape[0], true_figures.shape[1], -1)\n",
    "\n",
    "        predicted_figures = predicted_figures.permute(1, 0, 2, 3, 4)\n",
    "        predicted_figures_reshaped = predicted_figures.reshape(\n",
    "            predicted_figures.shape[0], predicted_figures.shape[1], -1\n",
    "        )\n",
    "\n",
    "        _, indexes = hungarian_slots_loss(figures_reshaped, predicted_figures_reshaped)\n",
    "        \n",
    "        indexes = torch.LongTensor(indexes)\n",
    "        predicted_latents = output[\"predicted_latents\"].detach().cpu()\n",
    "        true_latents = true_latents.detach().cpu()\n",
    "\n",
    "        # shuffling predicted latents to match true latents\n",
    "        predicted_latents = predicted_latents.gather(\n",
    "            1,\n",
    "            indexes[:, :, 1].unsqueeze(-1).expand(-1, -1, predicted_latents.shape[-1]),\n",
    "        )\n",
    "        latents.append(predicted_latents)\n",
    "\n",
    "latents = torch.cat(latents)\n",
    "# after this point we had z_hat matched to original, not permuted z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Permute predicted latents, and compare with permuted dataset\n",
    "latents = torch.cat([latents[:, 0].unsqueeze(1), latents[perms_concated, 1].unsqueeze(1)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: continued\n",
    "mse = 0\n",
    "batch_size_accum = 0\n",
    "with torch.no_grad():\n",
    "    for permuted_images, permuted_latents in permuted_dataloader:\n",
    "        true_figures = permuted_images[:, :-1, ...]\n",
    "        permuted_images = permuted_images[:, -1, ...].squeeze(1)\n",
    "        \n",
    "        output = decoder_hook(\n",
    "            latents[batch_size_accum : batch_size_accum + len(permuted_images)]\n",
    "        )\n",
    "        imagined_images = output[0]\n",
    "        \n",
    "        batch_size_accum += len(permuted_images)\n",
    "        # compare reconstructed images with imagined images\n",
    "\n",
    "        mse += ((permuted_images - imagined_images) ** 2).sum() / len(permuted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised model:  model_1_mse.mean()=2.0807428571428574 model_1_mse.std()=0.07174520107680378\n",
      "Supervised model:  model_2_mse.mean()=2.0907 model_2_mse.std()=0.06397577666585992\n",
      "SlotAttention model:  model_3_mse.mean()=13.92204 model_3_mse.std()=8.247602644793213\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "\n",
    "# Supervised model\n",
    "model_1_mse = np.array([2.0411, 2.0226, 2.1136, 2.1914, 2.0097, 2.0147, 2.1721])\n",
    "print(\"Supervised model: \", f\"{model_1_mse.mean()=}\",f\"{model_1_mse.std()=}\")\n",
    "\n",
    "# # Unsupervised model\n",
    "# model_2_mse = np.array([2.1929, 2.0903, 64.8243, 64.7224, 2.0591, 2.0205, 64.3132])\n",
    "# print(\"Supervised model: \", f\"{model_2_mse.mean()=}\",f\"{model_2_mse.std()=}\")\n",
    "\n",
    "# Unsupervised model\n",
    "model_2_mse = np.array([2.1929, 2.0903, 2.0591, 2.0205]) # failed seeds excluded\n",
    "print(\"Supervised model: \", f\"{model_2_mse.mean()=}\",f\"{model_2_mse.std()=}\")\n",
    "\n",
    "\n",
    "# SlotAttention model\n",
    "model_3_mse = np.array([7.4330, 20.6778, 26.6505, 5.7046, 9.1443])\n",
    "print(\"SlotAttention model: \", f\"{model_3_mse.mean()=}\",f\"{model_3_mse.std()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the debugging purposes\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(imagined_images.clip(0, 1)[1].permute(1, 2, 0))\n",
    "# plt.show()\n",
    "# plt.imshow(permuted_images[1].permute(1, 2, 0))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
